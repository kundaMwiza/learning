The source code provides a simple implementation of a naive and blocked implementation of matrix multiplication. Blocking is based on the GEPP & GEBP kernel detailed in [Anatomy of High-Performance Matrix Multiplication](https://faculty.csbsju.edu/mheroux/fall2013_csci317/GotoRVDGACMTOMS.pdf).

For large square matrices, the improvement in cache utilization that blocking enables results in a significant reduction in the run time of the matrix multiplication operation (see simple figure below). For both the blocked and naive implementation, the core kernels that compute the matrix multiplication are parallelized over 4 threads. For the blocked implementation, parallelization occurs within the GEBP kernel.

This figure is generated by obtaining 3 timings for each matrix size and taking the mean of the resulting times. The timings are obtained on a MacBook Pro (2017) which has a 2.3 GHz Dual-Core Intel Core i5. The main program is compiled with 03 optimisations and the required -fopenmp flag to enable OpenMP.

<p align="center">
  <img src="https://github.com/kundaMwiza/Learning/master/hpc/blas_impl/timings.png?raw=true" alt=""/>
</p>
