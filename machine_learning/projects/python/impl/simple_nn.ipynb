{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### elementary implementation of a simple neural network with 2 hidden layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# settings \n",
    "%matplotlib inline\n",
    "np.random.seed(1515)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),  (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "# convert mnist data to 2d\n",
    "x_train_nn = np.reshape(x_train, (60000, x_train.shape[1]**2))\n",
    "x_test_nn = np.reshape(x_test, (x_test.shape[0], x_test.shape[1]**2))\n",
    "empty_array = np.zeros(shape = (60000, 10))\n",
    "empty_array2 = np.zeros(shape = (x_test_nn.shape[0], 10))\n",
    "\n",
    "# brute force one hot encoding\n",
    "for i in range(60000):\n",
    "    empty_array[i, y_train[i]] = 1\n",
    "for i in range(x_test_nn.shape[0]):\n",
    "    empty_array2[i, y_test[i]] = 1\n",
    "\n",
    "y_train_2 = y_train[:]\n",
    "y_train = empty_array\n",
    "y_test = empty_array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 19429.436377, accuract: 0.902233\n",
      "loss: 14959.790563, accuract: 0.924600\n",
      "loss: 12832.460343, accuract: 0.935467\n",
      "loss: 11114.862115, accuract: 0.944167\n",
      "loss: 10057.775235, accuract: 0.949033\n",
      "loss: 9361.336589, accuract: 0.952667\n",
      "loss: 8121.620933, accuract: 0.958550\n",
      "loss: 7677.409774, accuract: 0.961633\n",
      "loss: 7375.470176, accuract: 0.962917\n",
      "loss: 6802.592994, accuract: 0.965933\n"
     ]
    }
   ],
   "source": [
    "def create_parameters(no_hidden_units):\n",
    "    \"\"\"\n",
    "    function to create parameters for 2 hidden layers\n",
    "    with corresponding bias parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    bw1 = np.random.normal(size = (no_hidden_units))\n",
    "    bw2 = np.random.normal(size = (10))\n",
    "    w1 = np.random.normal(size = (784, no_hidden_units))\n",
    "    w2 = np.random.normal(size = (no_hidden_units, 10))\n",
    "    return(w1, w2, bw1, bw2)\n",
    "\n",
    "w_1, w_2, bw1, bw2 = create_parameters(50)\n",
    "\n",
    "def sigmoid(w_1, bw1, x_train):\n",
    "    \"\"\"\n",
    "    sigmoid activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    a = np.dot(x_train.T, w_1) + bw1\n",
    "    hidden_units = (1+np.exp(-a))**-1\n",
    "    return hidden_units.T\n",
    "\n",
    "def propagate(w_1, w_2, bw1, bw2, x_train):\n",
    "    \"\"\"\n",
    "    propagate inputs through network\n",
    "    \"\"\"\n",
    "    hidden_units = sigmoid(w_1, bw1, x_train)\n",
    "    outputs = np.dot(hidden_units.T, w_2) + bw2\n",
    "    outputs -= np.max(outputs)\n",
    "    outputs = np.exp(outputs)\n",
    "    sum_top = np.sum(outputs, axis = 0)\n",
    "    outputs = outputs/sum_top\n",
    "    outputs += 10**-7\n",
    "    return hidden_units, outputs\n",
    "\n",
    "def backpropagate(w_1, w_2, bw1, bw2, x_train, y_train, no_hidden_units):\n",
    "    \"\"\"\n",
    "    calculate gradients through backpropagation\n",
    "    \"\"\"\n",
    "    hidden_units, outputs = propagate(w_1, w_2, bw1, bw2, x_train)\n",
    "    d2 = outputs - y_train\n",
    "    d2 = np.reshape(d2, (d2.shape[0], 1))\n",
    "    hidden_units = np.reshape(hidden_units, (hidden_units.shape[0], 1))\n",
    "    w2_grad = np.multiply(hidden_units, d2.T)\n",
    "    bw2_grad = d2\n",
    "    d1 = np.multiply(np.multiply(hidden_units, (1-hidden_units)), np.dot(w_2, d2))\n",
    "    d1 = np.reshape(d1, (d1.shape[0], 1))\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], 1))\n",
    "    w1_grad = np.multiply(x_train, d1.T)\n",
    "    bw1_grad = d1    \n",
    "    return(w1_grad, w2_grad, bw1_grad, bw2_grad)\n",
    "\n",
    "def update_weights(w_1, w_2, bw1, bw2, x_train, y_train, no_hidden_units):\n",
    "    \"\"\"\n",
    "    update weights using stochastic gradient descent\n",
    "    \"\"\"\n",
    "    s = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    x_train = x_train[s]\n",
    "    y_train = y_train[s]\n",
    "    for i in range(60000):\n",
    "        x_train_data = x_train[i, :]\n",
    "        y_train_data = y_train[i, :]\n",
    "        w1_grad, w2_grad, bw1_grad, bw2_grad = backpropagate(w_1, w_2, bw1, bw2, x_train_data, y_train_data, no_hidden_units)\n",
    "        w_1 = w_1 - 0.02*w1_grad\n",
    "        w_2 = w_2 - 0.02*w2_grad\n",
    "        bw1 = bw1 - 0.02*bw1_grad.flatten()\n",
    "        bw2 = bw2 - 0.02*bw2_grad.flatten()\n",
    "    return(w_1, w_2, bw1, bw2)\n",
    "\n",
    "def cross_entropy_loss(w_1, w_2, bw1, bw2, x_train, y_train, no_hidden_units):\n",
    "    obj = 0\n",
    "    counter = 0\n",
    "    for i in range(x_train.shape[0]):\n",
    "        y_pos = np.argmax(y_train[i, :])\n",
    "        hidden_units, outputs = propagate(w_1, w_2, bw1, bw2, x_train[i, :])\n",
    "        interm = -np.log(outputs[y_pos])\n",
    "        obj += interm\n",
    "        if np.argmax(outputs) == (y_pos):\n",
    "            counter += 1\n",
    "    return(obj, counter, counter/ x_train.shape[0])\n",
    "        \n",
    "def main(w_1, w_2, bw1, bw2, x_train, y_train, no_hidden_units):\n",
    "    \"\"\"\n",
    "    train network with 10 epochs\n",
    "    \"\"\"\n",
    "    for i in range(10):\n",
    "        w_1, w_2, bw1, bw2  = update_weights(w_1, w_2, bw1, bw2, x_train, y_train, no_hidden_units)\n",
    "        x1, _, x2 = cross_entropy_loss(w_1, w_2, bw1, bw2, x_train, y_train, no_hidden_units)\n",
    "        print(\"loss: %f, accuracy: %f\" % (x1, x2))\n",
    "    return(w_1, w_2, bw1, bw2)\n",
    "\n",
    "# get final weights\n",
    "w_1, w_2, bw1, bw2 = main(w_1, w_2, bw1, bw2, x_train_nn, y_train, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1769.120161146334, 9467, 0.9467)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy\n",
    "# objective, number of correct samples, and lastly accuracy\n",
    "cross_entropy_loss(w_1, w_2, bw1, bw2, x_test_nn, y_test, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper_env",
   "language": "python",
   "name": "build_central"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
