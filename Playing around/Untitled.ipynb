{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 38917.24\n",
      "loss: 58744.945\n",
      "loss: 72790.14\n",
      "loss: 56840.81\n",
      "loss: 40516.742\n",
      "loss: 14257.806\n",
      "loss: 6106.149\n",
      "loss: 5178.205\n",
      "loss: 3858.1577\n",
      "loss: 2999.3674\n",
      "loss: 3174.117\n",
      "loss: 3181.7986\n",
      "loss: 2449.3372\n",
      "loss: 2605.9304\n",
      "loss: 2794.349\n",
      "loss: 2635.9885\n",
      "loss: 2718.0625\n",
      "loss: 2157.5505\n",
      "loss: 2456.452\n",
      "loss: 2459.704\n",
      "loss: 2278.834\n",
      "loss: 2153.494\n",
      "loss: 2022.8223\n",
      "loss: 2133.757\n",
      "loss: 1888.113\n",
      "loss: 1924.1835\n",
      "loss: 1889.9498\n",
      "loss: 1966.1183\n",
      "loss: 2007.0677\n",
      "loss: 1959.0284\n",
      "loss: 2185.2463\n",
      "loss: 1769.13\n",
      "loss: 1945.8156\n",
      "loss: 2014.8717\n",
      "loss: 2054.782\n",
      "loss: 1694.856\n",
      "loss: 1255.3988\n",
      "loss: 1822.6709\n",
      "loss: 1544.977\n",
      "loss: 1998.4595\n",
      "loss: 1776.2633\n",
      "loss: 2062.3696\n",
      "loss: 1788.792\n",
      "loss: 1771.9038\n",
      "loss: 1733.1617\n",
      "loss: 1285.347\n",
      "loss: 1709.4102\n",
      "loss: 1724.458\n",
      "loss: 1484.7019\n",
      "loss: 1623.277\n",
      "loss: 1354.4503\n",
      "loss: 1804.9119\n",
      "loss: 1413.6827\n",
      "loss: 1319.0658\n",
      "loss: 1378.2976\n",
      "loss: 1566.4597\n",
      "loss: 1225.3802\n",
      "loss: 1577.4248\n",
      "loss: 1459.8595\n",
      "loss: 1266.8402\n",
      "accuracy on training set: 85.91499999999999\n",
      "loss: 1462.6482\n",
      "loss: 1301.2365\n",
      "loss: 1341.744\n",
      "loss: 1079.6265\n",
      "loss: 1238.7488\n",
      "loss: 1286.8602\n",
      "loss: 1428.3143\n",
      "loss: 1069.4967\n",
      "loss: 1092.7587\n",
      "loss: 1423.5112\n",
      "loss: 1376.5345\n",
      "loss: 1149.6011\n",
      "loss: 1203.796\n",
      "loss: 927.8883\n",
      "loss: 1221.3379\n",
      "loss: 998.3476\n",
      "loss: 1417.0343\n",
      "loss: 1149.3123\n",
      "loss: 1163.0356\n",
      "loss: 1282.9128\n",
      "loss: 1147.9728\n",
      "loss: 954.6923\n",
      "loss: 1252.9325\n",
      "loss: 984.71027\n",
      "loss: 1093.5425\n",
      "loss: 1069.0608\n",
      "loss: 1122.399\n",
      "loss: 1097.5714\n",
      "loss: 1108.9537\n",
      "loss: 1116.8053\n",
      "loss: 996.89105\n",
      "loss: 1190.9666\n",
      "loss: 1025.3251\n",
      "loss: 1112.0945\n",
      "loss: 994.92017\n",
      "loss: 859.82477\n",
      "loss: 1160.5879\n",
      "loss: 1063.2025\n",
      "loss: 887.5712\n",
      "loss: 944.20465\n",
      "loss: 977.8674\n",
      "loss: 747.89276\n",
      "loss: 815.6362\n",
      "loss: 863.4223\n",
      "loss: 1149.2798\n",
      "loss: 966.9674\n",
      "loss: 1178.5093\n",
      "loss: 1005.7506\n",
      "loss: 1031.3352\n",
      "loss: 709.33563\n",
      "loss: 896.17786\n",
      "loss: 1069.093\n",
      "loss: 858.77185\n",
      "loss: 954.5129\n",
      "loss: 903.38885\n",
      "loss: 1016.443\n",
      "loss: 1101.9182\n",
      "loss: 831.684\n",
      "loss: 1059.6648\n",
      "loss: 735.77673\n",
      "accuracy on training set: 88.47\n",
      "loss: 749.79047\n",
      "loss: 819.1505\n",
      "loss: 831.4907\n",
      "loss: 1031.464\n",
      "loss: 809.29114\n",
      "loss: 912.1895\n",
      "loss: 830.0685\n",
      "loss: 789.00055\n",
      "loss: 714.02606\n",
      "loss: 868.9932\n",
      "loss: 746.9426\n",
      "loss: 809.2019\n",
      "loss: 819.9234\n",
      "loss: 928.64056\n",
      "loss: 924.0898\n",
      "loss: 676.94354\n",
      "loss: 726.24664\n",
      "loss: 872.8834\n",
      "loss: 852.1602\n",
      "loss: 987.59357\n",
      "loss: 681.44354\n",
      "loss: 990.4815\n",
      "loss: 842.6059\n",
      "loss: 915.8319\n",
      "loss: 776.6153\n",
      "loss: 968.1485\n",
      "loss: 658.89886\n",
      "loss: 705.7861\n",
      "loss: 703.1327\n",
      "loss: 735.90424\n",
      "loss: 825.1943\n",
      "loss: 716.85254\n",
      "loss: 690.9129\n",
      "loss: 717.1604\n",
      "loss: 760.0934\n",
      "loss: 882.5659\n",
      "loss: 828.60785\n",
      "loss: 740.8856\n",
      "loss: 718.78864\n",
      "loss: 796.4609\n",
      "loss: 787.1491\n",
      "loss: 831.0223\n",
      "loss: 907.22864\n",
      "loss: 648.9974\n",
      "loss: 677.1977\n",
      "loss: 607.6004\n",
      "loss: 681.89014\n",
      "loss: 805.98285\n",
      "loss: 822.8627\n",
      "loss: 617.4013\n",
      "loss: 611.36914\n",
      "loss: 723.7091\n",
      "loss: 684.5245\n",
      "loss: 555.95715\n",
      "loss: 589.3801\n",
      "loss: 666.88446\n",
      "loss: 717.29065\n",
      "loss: 691.9893\n",
      "loss: 673.7777\n",
      "loss: 621.1303\n",
      "accuracy on training set: 89.81166666666667\n",
      "loss: 785.89386\n",
      "loss: 703.79596\n",
      "loss: 584.58484\n",
      "loss: 602.6835\n",
      "loss: 626.5758\n",
      "loss: 537.2921\n",
      "loss: 729.12634\n",
      "loss: 713.6691\n",
      "loss: 639.3501\n",
      "loss: 583.9939\n",
      "loss: 498.08554\n",
      "loss: 632.75024\n",
      "loss: 724.8259\n",
      "loss: 592.642\n",
      "loss: 657.5296\n",
      "loss: 520.4884\n",
      "loss: 609.46027\n",
      "loss: 455.37793\n",
      "loss: 665.17206\n",
      "loss: 500.12213\n",
      "loss: 679.77264\n",
      "loss: 552.64825\n",
      "loss: 669.3563\n",
      "loss: 707.20056\n",
      "loss: 718.442\n",
      "loss: 711.1943\n",
      "loss: 603.14685\n",
      "loss: 726.66876\n",
      "loss: 679.66614\n",
      "loss: 543.4637\n",
      "loss: 627.16876\n",
      "loss: 467.47488\n",
      "loss: 548.3877\n",
      "loss: 623.99664\n",
      "loss: 526.48615\n",
      "loss: 570.97687\n",
      "loss: 673.4428\n",
      "loss: 696.21265\n",
      "loss: 592.12024\n",
      "loss: 654.99426\n",
      "loss: 692.0401\n",
      "loss: 537.92126\n",
      "loss: 501.99606\n",
      "loss: 537.3569\n",
      "loss: 585.6936\n",
      "loss: 557.2905\n",
      "loss: 575.45294\n",
      "loss: 688.86084\n",
      "loss: 598.5061\n",
      "loss: 592.14276\n",
      "loss: 613.342\n",
      "loss: 481.05875\n",
      "loss: 506.6827\n",
      "loss: 506.0223\n",
      "loss: 631.9208\n",
      "loss: 530.88306\n",
      "loss: 527.9612\n",
      "loss: 663.7194\n",
      "loss: 653.1745\n",
      "loss: 582.3244\n",
      "accuracy on training set: 90.91333333333334\n",
      "loss: 512.67065\n",
      "loss: 672.1234\n",
      "loss: 532.9515\n",
      "loss: 526.6938\n",
      "loss: 567.0425\n",
      "loss: 588.4794\n",
      "loss: 544.26514\n",
      "loss: 536.2034\n",
      "loss: 561.60266\n",
      "loss: 552.94763\n",
      "loss: 477.12668\n",
      "loss: 562.51227\n",
      "loss: 555.1161\n",
      "loss: 565.7953\n",
      "loss: 484.38754\n",
      "loss: 469.18195\n",
      "loss: 394.6825\n",
      "loss: 538.0708\n",
      "loss: 688.72864\n",
      "loss: 495.52582\n",
      "loss: 553.487\n",
      "loss: 477.15588\n",
      "loss: 529.0639\n",
      "loss: 488.36298\n",
      "loss: 396.83096\n",
      "loss: 565.133\n",
      "loss: 448.403\n",
      "loss: 510.57352\n",
      "loss: 491.3196\n",
      "loss: 423.69962\n",
      "loss: 469.83325\n",
      "loss: 546.7263\n",
      "loss: 577.52454\n",
      "loss: 517.2813\n",
      "loss: 506.99963\n",
      "loss: 390.34256\n",
      "loss: 422.655\n",
      "loss: 578.0554\n",
      "loss: 395.96567\n",
      "loss: 426.47162\n",
      "loss: 503.148\n",
      "loss: 456.77042\n",
      "loss: 620.95874\n",
      "loss: 562.5514\n",
      "loss: 764.7496\n",
      "loss: 498.78323\n",
      "loss: 547.1781\n",
      "loss: 415.91205\n",
      "loss: 635.1397\n",
      "loss: 489.20984\n",
      "loss: 433.29797\n",
      "loss: 506.94702\n",
      "loss: 484.64374\n",
      "loss: 386.38943\n",
      "loss: 358.40976\n",
      "loss: 383.89355\n",
      "loss: 355.716\n",
      "loss: 373.99194\n",
      "loss: 463.5955\n",
      "loss: 467.77444\n",
      "accuracy on training set: 91.495\n",
      "accuracy on test set: 90.4\n"
     ]
    }
   ],
   "source": [
    "def createBatches(train, labels, n=1000):\n",
    "    l = len(train)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield train[ndx:min(ndx+n, l)], labels[ndx:min(ndx+n, l)]\n",
    "        \n",
    "        \n",
    "def MNISTclassification():\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, ftSize], name ='x')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, nDigits], name='y')\n",
    "    \n",
    "    with tf.variable_scope('yuna') as scope:\n",
    "        weightsOne = tf.Variable(np.random.normal(size=(ftSize, 300)), name='weightsOne', dtype=tf.float32)\n",
    "        h1 = tf.nn.relu(tf.matmul(x, weightsOne))\n",
    "#         weightsTwo = tf.Variable(np.random.normal(size=(300, 100)), name='weightsTwo', dtype=tf.float32)\n",
    "#         h2 = tf.nn.relu(tf.matmul(h1, weightsTwo))\n",
    "        weightsThree = tf.Variable(np.random.normal(size=(300, 10)), name='weightsThree', dtype=tf.float32)\n",
    "        h3 = tf.matmul(h1, weightsThree)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h3, labels=y))\n",
    "        correctPrediction = tf.equal(tf.argmax(h3, 1), tf.argmax(y, 1))\n",
    "        totalCorrect = tf.reduce_sum(tf.cast(correctPrediction, tf.float32))\n",
    "\n",
    "    return loss, x, y, totalCorrect, h3, weightsThree\n",
    "\n",
    "def run(xTrain, yTrain):\n",
    "    loss, x, y, correctPreds, preds, weights = MNISTclassification()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "    init = tf.global_variables_initializer\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init())\n",
    "        for i in range(5):\n",
    "            totalCorrect = 0\n",
    "            inds = np.random.choice(list(range(0, nTrain)), size=nTrain, replace=False)\n",
    "            xTrain = xTrain[inds]\n",
    "            yTrain = yTrain[inds]\n",
    "            for xBatch, yBatch in createBatches(xTrain, yTrain):\n",
    "                feedDict = {x: xBatch, y: yBatch}\n",
    "                lossVal, _ = sess.run([loss, optimizer], feedDict)\n",
    "                print('loss:', lossVal)\n",
    "            for xBatch, yBatch in createBatches(xTrain, yTrain):\n",
    "                feedDict = {x: xBatch, y: yBatch}\n",
    "                nCorrect = sess.run(correctPreds, feedDict)\n",
    "                totalCorrect += nCorrect\n",
    "            print('accuracy on training set:', (totalCorrect/nTrain)*100)\n",
    "        \n",
    "        feedDict = {x: xTest, y: yTest}\n",
    "        nCorrect, testPreds, weightval = sess.run([correctPreds, preds, weights], feedDict)\n",
    "        print('accuracy on test set:', (nCorrect/nTest)*100)\n",
    "    return testPreds, weightval\n",
    "            \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    (xTrain, yTrain), (xTest, yTest) = tf.keras.datasets.mnist.load_data()\n",
    "    nTrain = xTrain.shape[0]\n",
    "    nTest = xTest.shape[0]\n",
    "    nDigits = np.unique(yTrain).shape[0]\n",
    "    xTrain = xTrain.reshape(nTrain, 28*28)\n",
    "    xTest = xTest.reshape(nTest, 28*28)\n",
    "    yTrainTemp = np.zeros((nTrain, nDigits))\n",
    "    yTestTemp = np.zeros((nTest, nDigits))\n",
    "    yTrainTemp[np.arange(nTrain), yTrain] = 1\n",
    "    yTestTemp[np.arange(nTest), yTest] = 1\n",
    "    yTrain = yTrainTemp\n",
    "    yTest = yTestTemp\n",
    "    ftSize = xTrain.shape[-1]\n",
    "    testPred = run(xTrain, yTrain)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.48086661e-01,  3.09569925e-01, -9.98067617e-01, ...,\n",
       "        -9.63190317e-01, -7.16720298e-02, -1.12752414e+00],\n",
       "       [-1.43569684e+00, -8.42523575e-01, -1.20870495e+00, ...,\n",
       "        -8.77435923e-01, -1.51432857e-01, -7.63648570e-01],\n",
       "       [-3.70907843e-01, -8.17915797e-01,  1.15310915e-01, ...,\n",
       "         7.60201275e-01, -2.23801303e+00,  2.60063559e-01],\n",
       "       ...,\n",
       "       [-2.17038253e-03, -1.47296834e+00,  5.99841237e-01, ...,\n",
       "         1.48752189e+00,  9.85734582e-01, -9.57232475e-01],\n",
       "       [-1.35082173e+00,  2.10893098e-02,  5.89723885e-01, ...,\n",
       "         1.17312121e+00, -2.58577317e-01, -1.18239772e+00],\n",
       "       [-1.44440341e+00,  1.11571856e-01, -1.71515986e-01, ...,\n",
       "        -5.88924587e-01,  7.39001036e-01,  9.52517092e-01]], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTest.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcn_tensorflow",
   "language": "python",
   "name": "build_central"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
